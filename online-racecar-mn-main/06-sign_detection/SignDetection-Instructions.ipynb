{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions \n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from racecar_utils import *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ROS Node\n",
    "rospy.init_node('racecar')\n",
    "rc = Racecar()\n",
    "print('ROS node started successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Detection\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Remember the Feature Detection Lab? Now, let's create some functions that will help us <b style=\"color:green\">detect and track a one way sign</b> in a <b style=\"color:blue\">live video stream</b> using <b>ORB</b>!\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>get_keypoints</code>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    So that it is easier for us to find keypoints again in the future, let's create a function that creates an ORB object, detects image features, and returns the keypoints and descriptors! \n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Remember to create an ORB object using:\n",
    "    </p>\n",
    "\n",
    "```python\n",
    "orb = cv2.ORB_create(nFeatures)\n",
    "```\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Find the keypoints and descriptors of an image using:\n",
    "    </p>\n",
    "    \n",
    "```python\n",
    "keypoints, descriptors = orb.detectAndCompute(<grayscale_image>, None)\n",
    "```\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Finish the function below using ORB functions.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(image):\n",
    "    # TASK #1: Create an orb object\n",
    "\n",
    "    \n",
    "    # TASK #2: Detect and compute the image features/descriptors\n",
    "\n",
    "    \n",
    "    # TASK #3: Return the keypoints and descriptors using a tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>calculate_matches</code>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Now we need to <b style=\"color:blue\">match the keypoints</b> from the one way sign with keypoints from the video frames. \n",
    "    <br>We can use use <code>flann.knnMatch</code> to <b style=\"color:blue\">define the number of matches</b> we want.\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    It has the following format:\n",
    "    </p>\n",
    "    \n",
    "```python\n",
    "matches = flann.knnMatch(<description1>, <description2>, k=<num_best_matches>)\n",
    "```\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    After finding all the matches, we only want to keep the \"good\" matches. \n",
    "    <br> For an object to be truly \"detected\", the number (or percentage) of matches must be over a certain threshold. \n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Complete the <code>calculate_matches</code> function below by using <code>flann.knnMatch</code> and by determining what the \"detected\" threshold entails!\n",
    "    <br> Tweak the thresholds below to better tune your feature detector.\n",
    "    </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_TYPE = False            # (bool) True: use match count, False: use match percentage\n",
    "MIN_MATCH_COUNT = 10          # (int) minimum number of matches required for an object to be considered \"detected\"\n",
    "MIN_MATCH_PERCENTAGE = 0.02   # (float btwn 0-1) minimum match percentage required for an object to be considered \"detected\"\n",
    "\n",
    "print('nFeatures: {}, MIN_MATCH_COUNT: {}, MIN_MATCH_PERCENTAGE: {}'.format(nFeatures, MIN_MATCH_COUNT, MIN_MATCH_PERCENTAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matches(des_q, des_f):\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_LSH = 6\n",
    "    index_params= dict(algorithm = FLANN_INDEX_LSH, table_number = 6, key_size = 12, multi_probe_level = 1)\n",
    "    search_params = None\n",
    "\n",
    "    # Matching keypoints\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    if (des_q is not None) and (des_f is not None) and des_f.shape[0] > 10:\n",
    "        ## ------- Student Code Here ------- ##\n",
    "        # TASK #1: Use flann.knnMatch(). Save as 'matches'\n",
    "        #          Use des_q for <description1>, des_f for <description 2>. Use 2 for <num_best_matches>.\n",
    "\n",
    "        ## --------------------------------- ##\n",
    "    else:\n",
    "        matches = []\n",
    "\n",
    "    # store good matches via Lowe's ratio test\n",
    "    good_matches = []\n",
    "    for m_n in matches:\n",
    "        if len(m_n) != 2:\n",
    "            continue\n",
    "        (m,n) = m_n\n",
    "        if m.distance < 0.6*n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    total_matches = len(des_q) * 1.0\n",
    "    match_percentage = len(good_matches)/total_matches\n",
    "    print (\"# Matches: {}, Match Percentage: {}\".format(len(good_matches), match_percentage))\n",
    "    \n",
    "    detected = False\n",
    "\n",
    "    ## ------- Student Code Here ------- ##\n",
    "    # Checking if object is detected based on match count or percentage threshold\n",
    "    \n",
    "    # TASK #2: if MATCH_TYPE is true...\n",
    "\n",
    "        \n",
    "        # TASK #3: if the length of 'good_matches' (a list) is greater than MIN_MATCH_COUNT, set 'detected' to True.\n",
    "        #          else, set 'detected' to False\n",
    "        \n",
    "        \n",
    "            \n",
    "    # TASK #4: else...\n",
    "    \n",
    "        \n",
    "        # TASK #5: if 'match_percentage' is greater than MIN_MATCH_PERCENTAGE, set 'detected' to True.\n",
    "        # .        else, set 'detected' to False.\n",
    "        \n",
    "        \n",
    "            \n",
    "    ## --------------------------------- ##\n",
    "    \n",
    "    return detected, good_matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>detect_sign</code>\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    This function calculates the keypoints of the frame repeatedly and finds matches between the frame and the one way sign.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sign(frame, kp_q, des_q):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if (des_q is None):\n",
    "        detected = False\n",
    "        kp_q, kp_f = 0, 0\n",
    "        good_matches = []\n",
    "        return detected, kp_q, kp_f, good_matches\n",
    "\n",
    "    # TASK #1: Call the 'get_keypoints' function. Pass in 'frame' as argument.\n",
    "    #          Save the outputs as 'kp_f' and 'des_f'\n",
    "\n",
    "\n",
    "    # TASK #2: Call the 'calculate_matches' function from above. Pass in 'des_q' and 'des_f' as arguments.\n",
    "    #          Save the outputs as 'detected' and 'good_matches'\n",
    "\n",
    "    \n",
    "    return detected, kp_q, kp_f, good_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screen Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREEN_CENTER = 320.0     # pixel x-axis, camera is right-shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mark location of SCREEN_CENTER (x-axis)\n",
    "def identify_center(img):\n",
    "    if SCREEN_CENTER < 0 or SCREEN_CENTER > img.shape[1]:\n",
    "        print('SCREEN_CENTER out of bounds! Your image is: {}'.format(img.shape[:2]))\n",
    "    cv2.circle(img, (int(SCREEN_CENTER), img.shape[0]/2), 5, (0,255,0), 3)\n",
    "    return img\n",
    "\n",
    "# display image\n",
    "show_image(identify_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>detect_sign_live</code>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Let's put everything together! Complete the function below to detect and return the center coordinates of the one way sign.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True     # True: show video. (better to set as False during actual race to prevent video delays!)\n",
    "TEST_TIME = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup display\n",
    "if DEBUG:\n",
    "    display = IPython.display.display('', display_id=current_display_id)\n",
    "    current_display_id += 1\n",
    "query_columns = 0\n",
    "\n",
    "# TASK #1: Read \"one_way.jpg\"\n",
    "\n",
    "\n",
    "# TASK #2: Call 'get_keypoints', passing in the \"one_way.jpg\" we just read from above.\n",
    "#          Save the outputs as 'kp_q' and 'des_q'\n",
    "\n",
    "\n",
    "def detect_sign_live(frame):\n",
    "    global kp_q, des_q\n",
    "    \n",
    "    # TASK #3: Call 'detect_sign'. Figure ut the arguments you need to pass in, as well as the outputs you need.\n",
    "\n",
    "    \n",
    "    frame, dst, x_center, y_center = find_object(frame, queryImage, detected, kp_q, kp_f, good_matches, query_columns)\n",
    "    \n",
    "    if DEBUG:\n",
    "        # TASK #4: Draw a GREEN circle of radius 30 and thickness 5. \n",
    "\n",
    "        \n",
    "        cv2.polylines(frame,[np.int32(dst)], True, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "        show_frame(frame)\n",
    "\n",
    "rc.run(detect_sign_live, TEST_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Tune the following parameters to improve your sign detector!\n",
    "    <ul style='font-size:1.75rem;line-height:1.5'>\n",
    "        <li><code>nFeatures (int)</code>: maximum number of features we want to find in the image. The default value is 500.</li>\n",
    "        <li><code>MATCH_TYPE (bool)</code>: True: use MIN_MATCH_COUNT to threshold good matches, False: use MIN_MATCH_PERCENTAGE to threshold good matches</li>\n",
    "        <li><code>MIN_MATCH_COUNT (10)</code>: minimum number of matches required for an object to be considered \"detected\"</li>\n",
    "        <li><code>MIN_MATCH_PERCENTAGE ()</code>: minimum match percentage required for an object to be considered \"detected\"\n",
    "</li>\n",
    "    </ul>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Checkpoint\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    In order to attempt Sign Detection on the RACECAR, you MUST have completed SignDetection-Instructions (aka finish writing the functions above) and Cone Detection. This challenge relies largely on previously written code from the functions above, as well as experience with code manipulation from Cone Detection.\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    If you have NOT completed the SignDetection-Instructions and Cone Detection, please complete these labs first.\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Save Your Work!\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    You will need to <b>save your work</b> before continuing on to the next section:\n",
    "    <ul style='font-size:1.75rem;line-height:1.5'>\n",
    "        <li>Duplicate <code>\"ConeDetection.ipynb\"</code></li>\n",
    "        <li>Move the duplicate copy to your <code>/back-up</code> folder.</li>\n",
    "        <li>Duplicate <code>\"ConeDetection.ipynb\"</code> again</li>\n",
    "        <li>Rename the duplicate copy to <code>\"SignDetection.ipynb\"</code></li>\n",
    "        <li>Move the duplicate <code>\"SignDetection.ipynb\"</code> copy to the <code>/06-sign_detection</code> folder.</li>\n",
    "        <li>Continue working on your <code>\"SignDetection.ipynb\"</code> file in the <code>/06-sign_detection</code> folder.</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    The instructions above are <b>very important</b> so that you do not lose all of your hard work! \n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style='color:red'>Ask a TA for a check-off sticker before moving on to the next section.</b>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Live Object Detection: Traffic Signs\n",
    "\n",
    "## Overview\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    One other challenge from the final challenge is the ability to detect priority objects and drive towards it (without a line!). Correctly identifying objects will enable us to access even faster and shorter shortcuts! \n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    The objects that we will be identifying includes cones, traffic signs, and gateways. Now we will try detecting traffic signs (or more specifically, the \"One Way\" sign).\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Let's put everything together and make the racecars drive past the one way sign!\n",
    "    </p>\n",
    "    \n",
    "## Brainstorm Ideas Together!\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    After completing the lab above, you have all the tools needed to integrate sign detection into your final challenge.\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style='color:blue'>Discuss with your partner how you will approach integrating sign detection.</b>\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    Look over your current functions. What does each function do? What do you need to integrate sign detection?\n",
    "    </p>\n",
    "\n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b>BIG HINT:</b> The only thing you need is the center x coordinate from the detected object! Where is the center coordinate returned in the functions above? What functions do you need in order to get this center coordinate? How will the center coordinates of the object be used? (Another big hint: see the get_angle function).\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style='color:blue'>Discuss your plan of action with a TA first before getting started.</b>\n",
    "    </p>\n",
    "\n",
    "\n",
    "## Getting Started\n",
    " \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    There are many different ways to approach this problem. Start implementing your ideas after your team has agreed on a viable solution. Good luck!\n",
    "    </p>\n",
    "    \n",
    "<p style='font-size:1.75rem;line-height:1.5'>\n",
    "    <b style='color:red'>Remember to get a sticker from a TA before taking your car off the block!</b>\n",
    "    </p>\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
